import os
import json
import re
from typing import List, Dict, Optional
from dotenv import load_dotenv
from yandex_cloud_ml_sdk import YCloudML

load_dotenv()

class YandexDocumentIndexer:
    """–ö–ª–∞—Å—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ YandexGPT"""
    
    def __init__(self, folder_id: Optional[str] = None, api_key: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞
        
        Args:
            folder_id: ID –ø–∞–ø–∫–∏ Yandex Cloud
            api_key: API –∫–ª—é—á Yandex Cloud
        """
        self.folder_id = folder_id or os.getenv("YANDEX_FOLDER_ID")
        self.api_key = api_key or os.getenv("YANDEX_API_KEY")
        
        if not self.folder_id or not self.api_key:
            raise ValueError("–ù–µ —É–∫–∞–∑–∞–Ω folder_id –∏–ª–∏ api_key")
        
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è YandexGPT SDK...")
        self.sdk = YCloudML(folder_id=self.folder_id, auth=self.api_key)
        print("SDK —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤
        self.total_tokens = 0
    
    def split_text_into_chunks(self, text: str, chunk_size: int = 500, 
                               overlap: int = 50) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
        """
        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        chunks = []
        start = 0
        
        while start < len(text):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —á–∞–Ω–∫–∞
            end = start + chunk_size
            
            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫, –ø—ã—Ç–∞–µ–º—Å—è —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
            if end < len(text):
                # –ò—â–µ–º –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤ –æ–∫–Ω–µ
                sentence_end = max(
                    text.rfind('. ', start, end),
                    text.rfind('! ', start, end),
                    text.rfind('? ', start, end),
                    text.rfind('\n', start, end)
                )
                
                if sentence_end != -1:
                    end = sentence_end + 1
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # –°–¥–≤–∏–≥–∞–µ–º—Å—è —Å —É—á–µ—Ç–æ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            start = end - overlap if end < len(text) else end
        
        return chunks
    
    def generate_embeddings(self, chunks: List[str], 
                          model: str = "text-search-doc") -> List[List[float]]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —á–∞–Ω–∫–æ–≤ —á–µ—Ä–µ–∑ YandexGPT
        
        Args:
            chunks: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (text-search-doc –∏–ª–∏ text-search-query)
            
        Returns:
            –°–ø–∏—Å–æ–∫ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–µ–∫—Ç–æ—Ä–æ–≤)
        """
        print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(chunks)} —á–∞–Ω–∫–æ–≤...")
        
        embeddings = []
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            embedder = self.sdk.models.text_embeddings(model)
            
            for i, chunk in enumerate(chunks):
                try:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                    result = embedder.run(chunk)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
                    embedding_vector = result.embedding
                    embeddings.append(embedding_vector)
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç–æ–∫–µ–Ω–æ–≤
                    if hasattr(result, 'usage') and hasattr(result.usage, 'total_tokens'):
                        self.total_tokens += result.usage.total_tokens
                    
                    if (i + 1) % 10 == 0:
                        print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(chunks)} —á–∞–Ω–∫–æ–≤")
                
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —á–∞–Ω–∫–∞ {i}: {str(e)}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π –≤–µ–∫—Ç–æ—Ä –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                    embeddings.append([])
            
            print(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {self.total_tokens}")
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {str(e)}")
            raise
        
        return embeddings
    
    def create_index(self, text: str, chunk_size: int = 500, 
                    overlap: int = 50, model: str = "text-search-doc") -> Dict:
        """
        –°–æ–∑–¥–∞–µ—Ç –ø–æ–ª–Ω—ã–π –∏–Ω–¥–µ–∫—Å —Å —á–∞–Ω–∫–∞–º–∏ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
        
        Args:
            text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω–¥–µ–∫—Å–æ–º
        """
        # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏
        print("–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏...")
        chunks = self.split_text_into_chunks(text, chunk_size, overlap)
        print(f"–°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = self.generate_embeddings(chunks, model)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        index = {
            'metadata': {
                'total_chunks': len(chunks),
                'chunk_size': chunk_size,
                'overlap': overlap,
                'model': model,
                'embedding_dimension': len(embeddings[0]) if embeddings and embeddings[0] else 0,
                'total_tokens_used': self.total_tokens
            },
            'documents': [
                {
                    'id': i,
                    'text': chunk,
                    'embedding': embedding,
                    'char_start': sum(len(c) for c in chunks[:i]),
                    'char_end': sum(len(c) for c in chunks[:i+1])
                }
                for i, (chunk, embedding) in enumerate(zip(chunks, embeddings))
            ]
        }
        
        return index
    
    def save_index(self, index: Dict, output_path: str):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –≤ JSON —Ñ–∞–π–ª
        
        Args:
            index: –ò–Ω–¥–µ–∫—Å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            output_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
        """
        print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –≤ {output_path}...")
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(index, f, ensure_ascii=False, indent=2)
        print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
    
    def load_index(self, input_path: str) -> Dict:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏–∑ JSON —Ñ–∞–π–ª–∞
        
        Args:
            input_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –∏–Ω–¥–µ–∫—Å–æ–º
            
        Returns:
            –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å
        """
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ {input_path}...")
        with open(input_path, 'r', encoding='utf-8') as f:
            index = json.load(f)
        print("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")
        return index
    
    def process_file(self, input_filename: str, chunk_size: int = 500, 
                    overlap: int = 50, model: str = "text-search-doc") -> str:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª: —á–∏—Ç–∞–µ—Ç, —Å–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç
        
        Args:
            input_filename: –ò–º—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
            overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
            model: –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            
        Returns:
            –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É –∏–Ω–¥–µ–∫—Å–∞
        """
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        base_name = os.path.splitext(input_filename)[0]
        output_filename = f"{base_name}_index.json"
        
        print(f"\n{'='*60}")
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {input_filename}")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç –±—É–¥–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_filename}")
        print(f"{'='*60}\n")
        
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        try:
            with open(input_filename, 'r', encoding='utf-8') as f:
                text = f.read()
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {input_filename}\n")
        except FileNotFoundError:
            raise FileNotFoundError(f"–§–∞–π–ª {input_filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        except Exception as e:
            raise Exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        index = self.create_index(text, chunk_size, overlap, model)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞
        self.save_index(index, output_filename)
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print(f"\n{'='*60}")
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ù–î–ï–ö–°–ê")
        print(f"{'='*60}")
        print(f"–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {index['metadata']['total_chunks']}")
        print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {index['metadata']['embedding_dimension']}")
        print(f"–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {index['metadata']['chunk_size']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: {index['metadata']['overlap']} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {index['metadata']['total_tokens_used']}")
        print(f"–ú–æ–¥–µ–ª—å: {index['metadata']['model']}")
        
        if index['documents']:
            print(f"\n–ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–≥–æ —á–∞–Ω–∫–∞:")
            first_text = index['documents'][0]['text']
            print(first_text[:200] + ("..." if len(first_text) > 200 else ""))
            
            if index['documents'][0]['embedding']:
                print(f"\n–†–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {len(index['documents'][0]['embedding'])} —á–∏—Å–µ–ª")
                print(f"–ü–µ—Ä–≤—ã–µ 5 –∑–Ω–∞—á–µ–Ω–∏–π: {index['documents'][0]['embedding'][:5]}")
        
        print(f"{'='*60}\n")
        
        return output_filename


def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    
    # –ò–º—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    input_file = 'text_to_test.txt'
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –¥–µ–º–æ-—Ñ–∞–π–ª
    if not os.path.exists(input_file):
        print(f"‚ö†Ô∏è –§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—é –¥–µ–º–æ-—Ñ–∞–π–ª...")
        demo_text = """
        –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç - —ç—Ç–æ –æ–±–ª–∞—Å—Ç—å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã—Ö –Ω–∞—É–∫, –∫–æ—Ç–æ—Ä–∞—è –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è 
        —Å–æ–∑–¥–∞–Ω–∏–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö –º–∞—à–∏–Ω. –≠—Ç–∏ –º–∞—à–∏–Ω—ã —Å–ø–æ—Å–æ–±–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏, 
        –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ —Ç—Ä–µ–±—É—é—Ç —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞.
        """
        
        with open(input_file, 'w', encoding='utf-8') as f:
            f.write(demo_text)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω –¥–µ–º–æ-—Ñ–∞–π–ª {input_file}\n")
    
    try:
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞
        indexer = YandexDocumentIndexer()
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞
        output_file = indexer.process_file(
            input_filename=input_file,
            chunk_size=500,      # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            overlap=50,          # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            model="text-search-doc"  # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        )
        
        print(f"üéâ –ì–æ—Ç–æ–≤–æ! –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {str(e)}")


if __name__ == "__main__":
    main()